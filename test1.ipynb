{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 samples for language 0 and 2000 samples for language 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-11.512925 , -11.512925 , -11.512925 , ..., -11.512925 ,\n",
       "        -11.512925 , -11.512925 ],\n",
       "       [-11.512925 , -11.512925 , -11.512925 , ..., -11.512925 ,\n",
       "        -11.512925 , -11.512925 ],\n",
       "       [-11.512925 , -11.512925 , -11.512925 , ..., -11.512925 ,\n",
       "        -11.512925 , -11.512925 ],\n",
       "       ...,\n",
       "       [ -5.672583 ,  -6.862608 ,  -6.7738132, ...,  -6.6783767,\n",
       "         -7.703056 ,  -8.840825 ],\n",
       "       [ -6.2198935,  -6.979812 ,  -6.4984074, ...,  -6.659245 ,\n",
       "         -7.500062 ,  -8.744839 ],\n",
       "       [ -6.584874 ,  -7.218131 ,  -7.068345 , ...,  -6.657686 ,\n",
       "         -7.625363 ,  -8.85662  ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_npy_data_with_labels(folder_path, label):\n",
    "    data_list = []  # 用于存储所有读取的数据\n",
    "    labels_list = []  # 用于存储所有的标签\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            data = np.load(file_path)\n",
    "            data_list.append(data)\n",
    "            labels_list.append(label)\n",
    "    return data_list, labels_list\n",
    "\n",
    "# 使用你的文件夹路径替换这里\n",
    "base_folder_path = 'train_data'  # e.g., 'path_to_your_data/train_data'\n",
    "\n",
    "# 加载 language_0 的数据并分配标签 0\n",
    "language_0_folder_path = os.path.join(base_folder_path, 'language_0')\n",
    "language_0_data, language_0_labels = load_npy_data_with_labels(language_0_folder_path, 0)\n",
    "\n",
    "# 加载 language_1 的数据并分配标签 1\n",
    "language_1_folder_path = os.path.join(base_folder_path, 'language_1')\n",
    "language_1_data, language_1_labels = load_npy_data_with_labels(language_1_folder_path, 1)\n",
    "\n",
    "# 合并两种语言的数据和标签\n",
    "train_data_raw = language_0_data + language_1_data\n",
    "train_labels = language_0_labels + language_1_labels\n",
    "train_data_cropped = [matrix[:40] for matrix in train_data_raw]\n",
    "def min_max_normalize(tensor):\n",
    "    min_val = torch.min(tensor)\n",
    "    max_val = torch.max(tensor)\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor\n",
    "def standardize(tensor):\n",
    "    mean = torch.mean(tensor)\n",
    "    std = torch.std(tensor)\n",
    "    standardized_tensor = (tensor - mean) / std\n",
    "    return standardized_tensor\n",
    "# 打印出一些信息来确认数据已被加载\n",
    "print(f\"Loaded {len(language_0_data)} samples for language 0 and {len(language_1_data)} samples for language 1.\")\n",
    "train_data_cropped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5467, -0.5467, -0.5467,  ..., -0.5467, -0.5467, -0.5467],\n",
       "         [-0.5467, -0.5467, -0.5467,  ..., -0.5467, -0.5467, -0.5467],\n",
       "         [-0.5467, -0.5467, -0.5467,  ..., -0.5467, -0.5467, -0.5467],\n",
       "         ...,\n",
       "         [ 1.9457,  1.9139,  2.6182,  ...,  0.7078,  0.7386,  0.7527],\n",
       "         [ 1.9737,  1.9639,  2.5831,  ...,  0.8225,  1.0751,  1.0740],\n",
       "         [ 2.1232,  1.7222,  2.4205,  ...,  0.9448,  1.1420,  1.1041]]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建 TensorDataset 和 DataLoader\n",
    "train_dataset = TensorDataset(min_max_normalize(torch.tensor(train_data_cropped)), torch.tensor(train_labels))\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "# 随机分割成训练集和验证集\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=64,shuffle=True)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, nhead, num_layers, dropout=0.2):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(input_size)\n",
    "        self.input_fc = nn.Linear(input_size, hidden_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(hidden_size, nhead=nhead,dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.init_weights()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_padding_mask):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(self.input_fc(src), src_key_padding_mask=src_padding_mask)\n",
    "        output = self.fc(output.mean(dim=1))\n",
    "        out = torch.sigmoid(output).squeeze()\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (input_fc): Linear(in_features=80, out_features=128, bias=True)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch 1/100, Loss: 0.6929914951324463\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 2/100, Loss: 0.6930497884750366\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 3/100, Loss: 0.6934674382209778\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 4/100, Loss: 0.6931745409965515\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 5/100, Loss: 0.69303297996521\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 6/100, Loss: 0.6931205987930298\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 7/100, Loss: 0.6920985579490662\n",
      "Accuracy of the model on the validation data: 47 %\n",
      "Epoch 8/100, Loss: 0.672252893447876\n",
      "Accuracy of the model on the validation data: 63 %\n",
      "Epoch 9/100, Loss: 0.6717277765274048\n",
      "Accuracy of the model on the validation data: 73 %\n",
      "Epoch 10/100, Loss: 0.6469601988792419\n",
      "Accuracy of the model on the validation data: 72 %\n",
      "Epoch 11/100, Loss: 0.6266164183616638\n",
      "Accuracy of the model on the validation data: 69 %\n",
      "Epoch 12/100, Loss: 0.6558664441108704\n",
      "Accuracy of the model on the validation data: 63 %\n",
      "Epoch 13/100, Loss: 0.6396468877792358\n",
      "Accuracy of the model on the validation data: 70 %\n",
      "Epoch 14/100, Loss: 0.63327956199646\n",
      "Accuracy of the model on the validation data: 75 %\n",
      "Epoch 15/100, Loss: 0.6678568720817566\n",
      "Accuracy of the model on the validation data: 77 %\n",
      "Epoch 16/100, Loss: 0.6093535423278809\n",
      "Accuracy of the model on the validation data: 72 %\n",
      "Epoch 17/100, Loss: 0.5684903860092163\n",
      "Accuracy of the model on the validation data: 71 %\n",
      "Epoch 18/100, Loss: 0.6069475412368774\n",
      "Accuracy of the model on the validation data: 72 %\n",
      "Epoch 19/100, Loss: 0.5935137867927551\n",
      "Accuracy of the model on the validation data: 75 %\n",
      "Epoch 20/100, Loss: 0.5501127243041992\n",
      "Accuracy of the model on the validation data: 78 %\n",
      "Epoch 21/100, Loss: 0.6128497123718262\n",
      "Accuracy of the model on the validation data: 74 %\n",
      "Epoch 22/100, Loss: 0.5993402004241943\n",
      "Accuracy of the model on the validation data: 79 %\n",
      "Epoch 23/100, Loss: 0.5620354413986206\n",
      "Accuracy of the model on the validation data: 78 %\n",
      "Epoch 24/100, Loss: 0.6012201309204102\n",
      "Accuracy of the model on the validation data: 80 %\n",
      "Epoch 25/100, Loss: 0.5861188173294067\n",
      "Accuracy of the model on the validation data: 78 %\n",
      "Epoch 26/100, Loss: 0.5963575839996338\n",
      "Accuracy of the model on the validation data: 78 %\n",
      "Epoch 27/100, Loss: 0.5888079404830933\n",
      "Accuracy of the model on the validation data: 79 %\n",
      "Epoch 28/100, Loss: 0.5834842920303345\n",
      "Accuracy of the model on the validation data: 80 %\n",
      "Epoch 29/100, Loss: 0.6104995012283325\n",
      "Accuracy of the model on the validation data: 77 %\n",
      "Epoch 30/100, Loss: 0.5807498097419739\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 31/100, Loss: 0.6028100252151489\n",
      "Accuracy of the model on the validation data: 78 %\n",
      "Epoch 32/100, Loss: 0.5584179759025574\n",
      "Accuracy of the model on the validation data: 80 %\n",
      "Epoch 33/100, Loss: 0.6408308744430542\n",
      "Accuracy of the model on the validation data: 81 %\n",
      "Epoch 34/100, Loss: 0.5972304344177246\n",
      "Accuracy of the model on the validation data: 81 %\n",
      "Epoch 35/100, Loss: 0.5517920255661011\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 36/100, Loss: 0.5819928646087646\n",
      "Accuracy of the model on the validation data: 79 %\n",
      "Epoch 37/100, Loss: 0.6015871167182922\n",
      "Accuracy of the model on the validation data: 81 %\n",
      "Epoch 38/100, Loss: 0.600125253200531\n",
      "Accuracy of the model on the validation data: 81 %\n",
      "Epoch 39/100, Loss: 0.5763769149780273\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 40/100, Loss: 0.5787781476974487\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 41/100, Loss: 0.609432578086853\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 42/100, Loss: 0.5674877762794495\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 43/100, Loss: 0.5319802165031433\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 44/100, Loss: 0.5700523853302002\n",
      "Accuracy of the model on the validation data: 81 %\n",
      "Epoch 45/100, Loss: 0.5578191876411438\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 46/100, Loss: 0.5831104516983032\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 47/100, Loss: 0.5922333002090454\n",
      "Accuracy of the model on the validation data: 81 %\n",
      "Epoch 48/100, Loss: 0.5398780107498169\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 49/100, Loss: 0.5796982049942017\n",
      "Accuracy of the model on the validation data: 80 %\n",
      "Epoch 50/100, Loss: 0.56146639585495\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 51/100, Loss: 0.5935024619102478\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 52/100, Loss: 0.5566065311431885\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 53/100, Loss: 0.5840519666671753\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 54/100, Loss: 0.5800095200538635\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 55/100, Loss: 0.5380781888961792\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 56/100, Loss: 0.5187163352966309\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 57/100, Loss: 0.5267143249511719\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 58/100, Loss: 0.5513275265693665\n",
      "Accuracy of the model on the validation data: 80 %\n",
      "Epoch 59/100, Loss: 0.5522688627243042\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 60/100, Loss: 0.5508489012718201\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 61/100, Loss: 0.5261442065238953\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 62/100, Loss: 0.5203759074211121\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 63/100, Loss: 0.5337982177734375\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 64/100, Loss: 0.5742776393890381\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 65/100, Loss: 0.556537389755249\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 66/100, Loss: 0.5673417448997498\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 67/100, Loss: 0.5483264327049255\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 68/100, Loss: 0.5509264469146729\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 69/100, Loss: 0.5894668698310852\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 70/100, Loss: 0.5357574224472046\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 71/100, Loss: 0.5367835760116577\n",
      "Accuracy of the model on the validation data: 82 %\n",
      "Epoch 72/100, Loss: 0.5085015892982483\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 73/100, Loss: 0.5340884923934937\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 74/100, Loss: 0.49525201320648193\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 75/100, Loss: 0.5701100826263428\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 76/100, Loss: 0.5270008444786072\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 77/100, Loss: 0.5775697231292725\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 78/100, Loss: 0.5194147825241089\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 79/100, Loss: 0.5407435297966003\n",
      "Accuracy of the model on the validation data: 83 %\n",
      "Epoch 80/100, Loss: 0.5331079959869385\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 81/100, Loss: 0.5101217031478882\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 82/100, Loss: 0.5310497283935547\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 83/100, Loss: 0.5290018916130066\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 84/100, Loss: 0.5014232397079468\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 85/100, Loss: 0.5218222141265869\n",
      "Accuracy of the model on the validation data: 86 %\n",
      "Epoch 86/100, Loss: 0.5226762294769287\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 87/100, Loss: 0.5337934494018555\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 88/100, Loss: 0.49235281348228455\n",
      "Accuracy of the model on the validation data: 86 %\n",
      "Epoch 89/100, Loss: 0.538362979888916\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 90/100, Loss: 0.5164825916290283\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 91/100, Loss: 0.5706765055656433\n",
      "Accuracy of the model on the validation data: 86 %\n",
      "Epoch 92/100, Loss: 0.5131118297576904\n",
      "Accuracy of the model on the validation data: 86 %\n",
      "Epoch 93/100, Loss: 0.4735775589942932\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 94/100, Loss: 0.5647890567779541\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 95/100, Loss: 0.5095094442367554\n",
      "Accuracy of the model on the validation data: 86 %\n",
      "Epoch 96/100, Loss: 0.5330835580825806\n",
      "Accuracy of the model on the validation data: 85 %\n",
      "Epoch 97/100, Loss: 0.542819619178772\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 98/100, Loss: 0.5210317969322205\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 99/100, Loss: 0.5626679062843323\n",
      "Accuracy of the model on the validation data: 84 %\n",
      "Epoch 100/100, Loss: 0.5281246900558472\n",
      "Accuracy of the model on the validation data: 85 %\n"
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(input_size=80, hidden_size=128, num_classes=1, nhead=4, num_layers=4).to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二元分类\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs=100\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for sequence,label in train_loader:\n",
    "        sequence, label = sequence.to(device), label.to(device)\n",
    "        # 前向传播\n",
    "        label = label.float()\n",
    "        outputs = model(sequence,None)\n",
    "        loss = criterion(outputs, label)\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    total=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences,None)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    print('Accuracy of the model on the validation data: %d %%' % accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
